---
title: "p8105_hw6_zq2209"
author: "Zining Qi"
date: "2022-11-30"
output: github_document
---

```{r, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
```

# Problem 2

## Loading and cleaning data
```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide = read_csv(url) %>% 
  janitor::clean_names()
```

```{r}
homicide$city_state = paste(homicide$city, homicide$state, sep = ', ')
homicide %>%
  head() %>% 
  knitr::kable()
```

## Filtering data
```{r}
homicide_df = homicide %>% 
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL" ))) %>% 
  filter(victim_race %in% c("White", "Black"))
```

```{r}
homicide_df = homicide_df %>% 
  mutate(victim_age = ifelse(victim_age == "Unknown", NA, victim_age))
```

```{r}
homicide_df = homicide_df %>% 
  mutate(victim_age = strtoi(victim_age))
```

## Logistics regression for Baltimore, MD
```{r}
homicide_data = homicide_df %>% 
  mutate(solved_or_not = ifelse(disposition == "Closed by arrest", 1, 0))
```

```{r}
glm_baltimore = homicide_data %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(solved_or_not ~ victim_sex + victim_race + victim_age, family = binomial, data = .)

summary(glm_baltimore)
```

```{r}
save(glm_baltimore, file = "result/glm_baltimore_result.RData")
```

### Odds ratio and confidence interval of odds ratio
```{r}
glm_baltimore_table = broom::tidy(glm_baltimore) %>% 
  mutate(or = exp(estimate), 
         or_lower = exp(estimate - 1.96*std.error), 
         or_upper = exp(estimate + 1.96*std.error)) %>% 
  filter(term == "victim_sexMale") %>% 
  select(or, or_lower, or_upper) %>% 
  rename("Odds ratio" = or, 
         "Lower bound" = or_lower, 
         "Upper bound" = or_upper) %>% 
  knitr::kable(digits = 3)

glm_baltimore_table
```

The odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed in Baltimore is 0.426, and 95% confidence interval is (0.325, 0.558).


## Odds ratio and confidence interval for all cities
```{r}
glm_all = function(citystate){
  city_glm = homicide_data %>% 
    filter(city_state == citystate) %>% 
    glm(solved_or_not ~ victim_sex + victim_race + victim_age, family = binomial, data = .) %>% 
    broom::tidy() %>% 
    mutate(or = exp(estimate), 
         or_lower = exp(estimate - 1.96*std.error), 
         or_upper = exp(estimate + 1.96*std.error)) %>% 
    filter(term == "victim_sexMale") %>% 
    select(or, or_lower, or_upper)
    
    city_glm
}
```

```{r}
city_state_list = homicide_data %>% 
  select(city_state) %>% 
  unique()
```

```{r}
glm_all_result = city_state_list %>% 
  mutate(glm_result = map(city_state, glm_all)) %>% 
  unnest(glm_result) %>% 
  arrange(desc(or))
```

### Table of odds ratio for all cities
```{r}
glm_all_table = glm_all_result %>% 
  rename("Odds ratio" = or, 
         "Lower bound" = or_lower, 
         "Upper bound" = or_upper) %>% 
  knitr::kable(digits = 3)

glm_all_table
```

### Graph that show the odds ratio and confidence interval for all cities
```{r}
glm_all_result %>% 
  mutate(city_state = fct_reorder(city_state, or)) %>% 
  ggplot(aes(x = city_state, y = or)) + 
  geom_point(color = "red") + 
  geom_errorbar(aes(ymin = or_lower, ymax = or_upper)) + 
  coord_flip() + 
  labs(title = "Odds Ratio of solving cases comparing male victims to female victims", 
       y = "Odds ratio of solving cases", 
       x = "City, State", 
       caption = "Bars represent 95% confidence interval") + 
  theme_classic() 
```

From the plot of odds ratio and confidence interval, Albuquerque has the largest odds ratio(larger than 1) and confidence interval, while New York has the smallest(smaller than 1). Confidence interval that doesn't include 1 implies 95% confidence that there is a difference for solving homicides comparing male victims to female victims. The smaller the odds ratio, the largest differences in solving homicides comparing male victims to female victims.


# Problem 3
## LOading and cleaning data
```{r}
birthweight = read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names()

birthweight = birthweight %>% 
  mutate(babysex = as.factor(babysex), 
         frace = as.factor(frace), 
         malform = as.factor(malform), 
         mrace = as.factor(mrace))
```

### Histogram of birth weight
```{r}
birthweight_hist = birthweight %>% 
  ggplot(aes(x = bwt)) + 
  geom_histogram() + 
  labs(x = "Birthweight")

birthweight_hist
```


## Propose a model
From the course, Introduction to Public Health, I have learned some factors that influence birth weight of new born, and read some articles about the issue. From these information, there are some factors that affect birth weight a lot. These are biology factor: mothers' race, mothers' health condition, mothers' age. And there are social and financial factors, such as income of the family. Basrs on these information, the predictors that I choose are `Length of Pregnancy, Mother's birth weight, Age of the parent, race, SES (income)`. The model would be `bwt ~ gaweeks + mrace + ppbmi + fincome + momage`.

```{r}
lm_bwt = lm(bwt ~ gaweeks+mrace+ppbmi+fincome+momage, data = birthweight)
```

### Summary of the model
```{r}
lm_bwt %>% broom::tidy()
```

### Residual plot of the model
```{r}
birthweight %>% 
  modelr::add_predictions(lm_bwt) %>% 
  modelr::add_residuals(lm_bwt) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(se = FALSE, method = "lm", color = "red") +
  labs(
    x = "Predicted Values",
    y = "Residuals",
    title = "Residuals vs. predicted values"
  )
```

The plot seems okay because the red line is horizontal, and the plot seems random with predicted values.

## Two other models
```{r}
lm_main = lm(bwt ~ gaweeks+blength, data = birthweight)
lm_inter = lm(bwt ~ bhead*blength*babysex, data = birthweight)
```

```{r}
lm_main %>% broom::tidy()
lm_inter %>% broom::tidy()
```

## Comparing three models using cross validation
```{r}
cv_df = 
  crossv_mc(birthweight, 100)
```

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    lm_bwt  = map(train, ~lm(bwt ~ gaweeks+mrace+ppbmi+fincome+momage, data = .x)),
    lm_main  = map(train, ~lm(bwt ~ gaweeks+blength, data = .x)),
    lm_inter  = map(train, ~lm(bwt ~ bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_bwt = map2_dbl(lm_bwt, test, ~rmse(model = .x, data = .y)),
    rmse_main = map2_dbl(lm_main, test, ~rmse(model = .x, data = .y)),
    rmse_inter = map2_dbl(lm_inter, test, ~rmse(model = .x, data = .y)))
```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin(aes(fill = model)) +
  theme_bw() +
  labs(
    x = "Model",
    y = "RMSE",
    title = "RMSE distribution across 3 models"
  )
```

The three-interaction model seems the best among three models according to the RMSE distribution for each model. The model with all interaction term has the smallest prediction error compared to others. So, the model with interactions are better.



